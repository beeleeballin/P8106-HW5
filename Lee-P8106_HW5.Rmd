---
title: "P8106 HW5"
author: "Brian Jo Hsuan Lee"
date: "4/28/2022"
output: pdf_document
---

Load packages
```{r}
library(tidyverse)
library(caret)
library(ISLR)
library(factoextra)
```

## Problem 1: Auto Classifier using Support Vector Machine

Load and tidy auto data, and split it into training and testing sets
```{r}
data = read_csv("auto.csv", col_types = "fdiidfff") %>% 
  mutate(
    cylinders = fct_relevel(cylinders, "3", "4", "5", "6", "8"),
    origin = fct_relevel(origin, "1", "2", "3")
  )

set.seed(2022)
rowTrain = createDataPartition(y = data$mpg_cat,
                               p = 0.7,
                               list = FALSE)
```

Set 10 fold cross validation to optimize tuning parameters 
```{r}
ctrl = trainControl(method = "cv")
```

a) 
**Support Vector Classifier with Linear Kernel**

The following is a fit using a linear kernel implemented by the `kernlab` package
```{r}
data.frame(C = exp(seq(-5, 2, len = 50)))

# kernlab
set.seed(2022)
svml.fit = train(mpg_cat ~ . , 
                 data = data[rowTrain,], 
                 method = "svmLinear",
                 # preProcess = c("center", "scale"),
                 tuneGrid = data.frame(C = exp(seq(-5, 2, len = 50))),
                 trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)
```

b) 
**Support Vector Classifier with Radial Kernel**

The following is a fit using a radial kernel but tuning both over both cost and sigma.
```{r, fig.width=15, fig.height=8}
svmr.grid = expand.grid(C = exp(seq(-1, 4, len = 20)),
                        sigma = exp(seq(-9, 0, len = 50)))

set.seed(2022)             
svmr.fit = train(mpg_cat ~ . ,
                 data = data, 
                 subset = rowTrain,
                 method = "svmRadialSigma",
                 tuneGrid = svmr.grid,
                 trControl = ctrl)

myCol = rainbow(20)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

plot(svmr.fit, highlight = TRUE, par.settings = myPar)
```

```{r}
resamp = resamples(list(svml = svml.fit, svmr = svmr.fit))
bwplot(resamp)
```

We finally look at the test data performance.
```{r}
pred.svml = predict(svml.fit, newdata = data[-rowTrain,])
pred.svmr = predict(svmr.fit, newdata = data[-rowTrain,])

confusionMatrix(data = pred.svml, 
                reference = data$mpg_cat[-rowTrain])

confusionMatrix(data = pred.svmr, 
                reference = data$mpg_cat[-rowTrain])
```

## Problem 2: US State Classifier using Hierarchical Clustering 
 
Load US Arrest dataset
```{r}
data2 = USArrests
```

a) 
**Use hierarchical clustering with complete linkage and Euclidean distance for state clustering.**

Present the 3 cluster dendrogram
```{r}
# compute the 3 clusters of states
hc_complete = hclust(dist(data2), method = "complete")

# display
fviz_dend(hc_complete, k = 3,        
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)
```

List the state belong in each cluster
```{r}
# compute the 3 clusters of states (basically the above dendrogram in another format)
state_clusters = cutree(hc_complete, 3)

# record the states in each cluster
cl1 = row.names(data2[state_clusters == 1,])
cl2 = row.names(data2[state_clusters == 2,])
cl3 = row.names(data2[state_clusters == 3,])

# create a table to display cluster information
table_height = max(length(cl1), length(cl2), length(cl3))
cluster_table = data.frame(matrix(ncol = 3, nrow = table_height))
colnames(cluster_table) = c("Cluster 1", "Cluster 2", "Cluster 3")
cluster_table[1:length(cl1), 1] = cl1
cluster_table[1:length(cl2), 2] = cl2
cluster_table[1:length(cl3), 3] = cl3

# display
knitr::kable(cluster_table, "simple")
```

b) 
**Standardize all the covariates for state prediction, and repeat the hierarchical clustering**

```{r}
# standardize (transform all within-variable variance to 1) the data
data2_std = 
  USArrests %>% 
  mutate_all(~(scale(.) %>% as.vector))

# compute and display the 3 clusters of states
hc_complete_std = hclust(dist(data2_std), method = "complete")

fviz_dend(hc_complete_std, k = 3,        
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

state_clusters_std = cutree(hc_complete_std, 3)
cl1_std = row.names(data2[state_clusters_std == 1,])
cl2_std = row.names(data2[state_clusters_std == 2,])
cl3_std = row.names(data2[state_clusters_std == 3,])

table_height_std = max(length(cl1_std), length(cl2_std), length(cl3_std))
cluster_table_std = data.frame(matrix(ncol = 3, nrow = table_height_std))
colnames(cluster_table_std) = c("Cluster 1", "Cluster 2", "Cluster 3")
cluster_table_std[1:length(cl1_std), 1] = cl1_std
cluster_table_std[1:length(cl2_std), 2] = cl2_std
cluster_table_std[1:length(cl3_std), 3] = cl3_std

knitr::kable(cluster_table_std, "simple")
```

